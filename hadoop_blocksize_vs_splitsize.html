<!doctype html>
<html lang="">	
<head>
	<meta charset="utf-8"/>
	<title>Hadoop的block Size和split Size是什么关系 - 仗梦天涯&nbsp;&nbsp;&nbsp;&nbsp;倚霞而栖</title>	
	<meta name="author" content="Poon">
	
	<meta name="HandheldFriendly" content="True">
	<meta name="MobileOptimized" content="320">
	<meta name="viewport" content="width=device-width, target-densitydpi=160dpi, initial-scale=1.0">
	
	<link href='http://fonts.googleapis.com/css?family=Droid+Sans:700,400|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" href="/theme/css/main.css" type="text/css" />
		
	<!--[if lt IE 9]>
		<script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
	
	
	<link href="/" type="application/atom+xml" rel="alternate" title="仗梦天涯&nbsp;&nbsp;&nbsp;&nbsp;倚霞而栖 ATOM Feed" />
</head>
	
<body>		
	<header class="clearfix" role="banner">
		<div class="wrapper">
			<h1 class="huge"><a href="">仗梦天涯&nbsp;&nbsp;&nbsp;&nbsp;倚霞而栖</a></h1>
		</div>
	</header>
	
<div role="main" class="content clearfix">	
	<article>
		<div class="post wrapper">
			<h1>Hadoop的block Size和split Size是什么关系</h1>
			<p>学习hadoop map reduce过程的时候，第一步就是split。我们知道，hdfs中的数据是按block来存储的。问题来了，那么split和block之间是什么关系呢？我google到了stackoverflow上的这篇文章，我觉得这个帖子应该把关系说清楚了，翻译出来，欢迎大家批评指正！以下：</p>
<h2 id="_1">问题</h2>
<p>hadoop的split size 和 block size 是什么关系？ 是否 split size 应该 n倍于 block size ? </p>
<h2 id="_2">概念</h2>
<p>在 hdfs 架构中，存在 blocks 的概念。 通常来说，hdfs中的一个block 是 64MB 。 当我们把一个大文件导入hdfs中的时候，文件会按 64MB 每个block来分割（如果按默认配置）。
如果你有1GB的文件要存入HDFS中， 1GB/64MB = 1024MB / 64MB  = 16 个blocks 会被分割到不同的datanode上。</p>
<h2 id="_3">目的</h2>
<p>数据分割(data splitting )策略是基于文件偏移进行的。文件分割的目的是 有利于数据并行处理 ，以及 便于数据容灾恢复。</p>
<h2 id="_4">区别</h2>
<p>split 是逻辑意义上的split。 通常在 M/R 程序或者其他数据处理技术上用到。根据你处理的数据量的情况，split size是允许用户自定义的。</p>
<p>split size 定义好了之后，可以控制 M/R 中 Mapper 的数量。如果M/R中没有定义 split size ， 就用 默认的HDFS配置作为 input split。</p>
<h2 id="_5">举例</h2>
<p>你有个100MB的文件，block size 是 64MB ， 那么就会被split成 2 块。这时如果你你没有指定 input split ， 你的M/R程序就会按2个input split 来处理 ， 并分配两个mapper来完成这个job。</p>
<p>但如果你把 split size 指定为 100MB，那么M/R程序就会把数据处理成一个 split，这时只用分配一个mapper 就可以了。</p>
<p>但如果你把split size 指定为 25MB，M/R就会将数据分成4个split，分配4个mapper来处理这个job。</p>
<h2 id="_6">总结</h2>
<ol>
<li>block是物理上的数据分割，而split是逻辑上的分割。</li>
<li>如果没有特别指定，split size 就等于 HDFS 的 block size 。</li>
<li>用户可以在M/R 程序中自定义split size。</li>
<li>一个split 可以包含多个blocks，也可以把一个block应用多个split操作。</li>
<li>有多少个split，就有多少个mapper。</li>
</ol>
<p>原文：</p>
<p>```</p>
<p>Q: What is relationship between split size and block size in Hadoop? As I read in this, split size must be n-times of block size (n is an integer and n &gt; 0), is this correct? Is there any must in relationship between split size and block size? **</p>
<p>A:
In HDFS architecture there is a concept of blocks. A typical block size used by HDFS is 64 MB. When we place a large file into HDFS it chopped up into 64 MB chunks(based on default configuration of blocks), Suppose you have a file of 1GB and you want to place that file in HDFS,then there will be 1GB/64MB = 16 split/blocks and these block will be distribute across the DataNodes. These blocks/chunk will reside on a different DataNode based on your cluster configuration.</p>
<p>Data splitting happens based on file offsets.The goal of splitting of file and store it into different blocks is parallel processing and fail over of data.</p>
<p>Difference between block size and split size.</p>
<p>Split is logical split of your data, basically used during data processing using Map/Reduce program or other processing techniques. Split size is user defined and you can choose your split size based on your data(How much data you are processing).</p>
<p>Split is basically used to control number of Mapper in Map/Reduce program. If you have not defined any input split size in Map/Reduce program then default HDFS block split will be considered as input split.</p>
<p>Example:</p>
<p>Suppose you have a file of 100MB and HDFS default block configuration is 64MB then it will chopped in 2 split and occupy 2 blocks. Now you have a Map/Reduce program to process this data but you have not specified any input split then based on the number of blocks(2 block) input split will be considered for the Map/Reduce processing and 2 mapper will get assigned for this job.</p>
<p>But suppose,you have specified the split size(say 100MB) in your Map/Reduce program then both blocks(2 block) will be considered as a single split for the Map/Reduce processing and 1 Mapper will get assigned for this job.</p>
<p>Suppose,you have specified the split size(say 25MB) in your Map/Reduce program then there will be 4 input split for the Map/Reduce program and 4 Mapper will get assigned for the job.</p>
<p>Conclusion:</p>
<ol>
<li>Split is a logical division of the input data while block is a physical division of data.</li>
<li>HDFS default block size is default split size if input split is not specified.</li>
<li>Split is user defined and user can control split size in his Map/Reduce program.</li>
<li>One split can be mapping to multiple blocks and there can be multiple split of one block.</li>
<li>The number of map tasks (Mapper) are equal to the number of splits.</li>
</ol>
<p>ref: http://stackoverflow.com/questions/30549261/split-size-vs-block-size-in-hadoop</p>
<p>```</p>
<h2 id="_7">补充</h2>
<h3 id="hdfs">hdfs中如何设置参数</h3>
<h4 id="split-size">split size 配置：</h4>
<blockquote>
<p>mapred.max.split.size </p>
</blockquote>
<h4 id="block-size">block size 配置：</h4>
<blockquote>
<p>dfs.block.size </p>
</blockquote>
<p>注意，请不要轻易更改 block size,因为这个是影响整个hdfs的。</p>
<h4 id="split-size_1">split size 计算规则：</h4>
<p>针对 ：
mapreduce.input.fileinputformat.split.maxsize
mapreduce.input.fileinputformat.split.minsize
FileInputFormat.computeSplitSize()</p>
<p>按如下规则取 split size : </p>
<p>Math.max(minSize, Math.min(maxSize, blockSize))</p>
<h4 id="_8">性能</h4>
<ol>
<li>一般来说，block size 和 split size 设置成一致，性能较好。</li>
<li>FileInputFormat generates splits in such a way that each split is all or part of a single file. 所以 hadoop处理大文件比处理小文件来得效率高得多。</li>
</ol>
<p><img alt="bottom求关注" src="https://mmbiz.qlogo.cn/mmbiz/sfKia69cLy1yGH30FHU6SYaJPqvibh7Wib9Pg2V6rc7zjaPJ7aKk9NcpQb9IIhZLCIG8CB4b0QV2vKWopevlhvafw/0?wx_fmt=png" /></p>
			
			<a href="https://twitter.com/share" class="twitter-share-button" data-via="" data-lang="en" data-size="large" data-related="">Tweet</a>
			<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
						
			<div class="comments">
			<h2>Comments !</h2>
			    <div id="disqus_thread"></div>
			    <script type="text/javascript">
			       var disqus_identifier = "hadoop_blocksize_vs_splitsize.html";
			       (function() {
			       var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			       dsq.src = '//gitwillpoonimx3.disqus.com/embed.js';
			       (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
			      })();
			    </script>
			</div>
			
		</div>
	
<div class="meta wrapper">
	<time datetime="2016-04-04T12:32:46+08:00" pubdate>Mon 04 April 2016</time>
	<ul class="tag clearfix">
		<li><a href="/category/misc.html">misc</a></li>
	</ul>
</div>	</article>	
</div>
	
		
<footer class="clearfix">
	<div class="wrapper pages">
		<ul class="nav">
			<li><a href="/archives.html">Archive</a></li>
		</ul>
	</div>
	
	<div class="copy wrapper">
		<ul class="social">
			<li><a href="http://weibo.com/poooon">See me on Sina WeiBo</a></li>
			<li><a href="http://github.com/willpoon">See me on GitHub</a></li>
		</ul>
	
		<p role="contentinfo">© 2012 Poon<br>
		Proudly powered by <a href="http://alexis.notmyidea.org/pelican/">Pelican</a>.</p>
	</div>
</footer>
	
	<script>
	  var _gaq=[['_setAccount','UA-69868761-1'],['_trackPageview']];
	  (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
	  g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
	  s.parentNode.insertBefore(g,s)}(document,'script'));
	</script>
</body>
</html>